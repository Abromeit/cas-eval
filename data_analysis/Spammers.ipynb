{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('__file__'), os.path.pardir)))\n",
    "\n",
    "import third_party.krippendorff_alpha.krippendorff_alpha as krippendorff_alpha\n",
    "\n",
    "import logs_processing.cohen_kappa as cohen_kappa\n",
    "from logs_processing.fields import free_text_fields, non_english, orig_query, rel_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CF_TRUST = False\n",
    "\n",
    "DUMP_AGGREGATED_SCORES = False\n",
    "PRINT_PER_WORKER_SCORES = False\n",
    "\n",
    "SKIP_INCONSISTENT_WORKERS = False\n",
    "MALICIOUS_WORKER_THRESHOLD = 0.3\n",
    "SUSPICIOUS_WORKER_THRESHOLD = 0.66\n",
    "MIN_JUDGEMENTS_PER_WORKER = 3\n",
    "\n",
    "SKIP_UNCLEAR_ITEMS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF = '<DIRECTORY_WITH_EXPORTED_CROWD_FLOWER_RESULT_CSVs>'\n",
    "INPUTS = ['f842336_A+R.part1.csv', 'f845369_A+R.part2.csv', 'f845808_A+R.part3.csv', 'f846814_A+R.part4.csv']\n",
    "MODE = 'R' # 'D', 'A' or 'R'\n",
    "DICTIONARY = '<DICTIONARY_OF_ENGLISH_WORDS.txt>'\n",
    "DAYS = None  # limit to some days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_inconsistent(row):\n",
    "    bad_abd = 'bad_abandonment' in row['query'].split()\n",
    "    if bad_abd and row['main'] in ['D1', 'D2']:\n",
    "        return True\n",
    "    if bad_abd and row['no_detailed'] == 'D0.2':\n",
    "        return True\n",
    "\n",
    "def is_malicious(row, mode, snippet_text):\n",
    "    if row['main'] in ['D1', 'D2']:\n",
    "        # TODO: fetch the doc and check the detailed answer for R.\n",
    "        for token in row['yes_detailed'].split():\n",
    "            if token in snippet_text:\n",
    "                return False\n",
    "        return True\n",
    "    elif row['main'] == ('D-1' if mode == 'D' else 'A-1'):\n",
    "        return row[non_english[mode]] not in row[orig_query[mode]]\n",
    "    elif row['main'] == ('D-2' if mode == 'D' else 'A-2'):\n",
    "        for token in row[non_english[mode]].split():\n",
    "            if token in snippet_text:\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "dictionary_contents = set()\n",
    "with open(DICTIONARY) as f:\n",
    "    dictionary_contents = set(line.rstrip() for line in f)\n",
    "    \n",
    "def is_suspicious(row):\n",
    "    for f in free_text_fields:\n",
    "        text = row.get(f, '')\n",
    "        if len(text) > 0 and all(t.upper() not in dictionary_contents for t in text.split()):\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for input_f in INPUTS:\n",
    "    with open(os.path.join(CF, input_f)) as f:\n",
    "        rows += list(csv.DictReader(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go over the data to count workers' mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workers = collections.defaultdict(lambda: 0)\n",
    "inconsistent_workers = collections.defaultdict(lambda: 0)\n",
    "malicious_workers = collections.defaultdict(lambda: 0)\n",
    "suspicious_workers = collections.defaultdict(lambda: 0)\n",
    "log_id_to_snippet_text = {}\n",
    "for row in rows:\n",
    "    if DAYS is not None and row['_started_at'].split()[0] not in DAYS:\n",
    "        continue\n",
    "    q = row[orig_query[MODE]]\n",
    "    worker_id = row['_worker_id']\n",
    "    workers[worker_id] += 1\n",
    "    if MODE == 'D' and is_inconsistent(row):\n",
    "        inconsistent_workers[worker_id] += 1\n",
    "        \n",
    "    snippet = bs4.BeautifulSoup(row['snippet'], 'lxml')\n",
    "    snippet_text = snippet.get_text().encode('utf-8')\n",
    "    log_id_to_snippet_text[row['log_id']] = snippet_text\n",
    "    if is_malicious(row, MODE, snippet_text):\n",
    "        malicious_workers[worker_id] += 1\n",
    "    if is_suspicious(row):\n",
    "        suspicious_workers[worker_id] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized(d):\n",
    "    return collections.defaultdict(lambda: 0, {k: (v / workers[k]) for k, v in d.iteritems() if workers[k] >= MIN_JUDGEMENTS_PER_WORKER})\n",
    "\n",
    "inconsistent_workers = normalized(inconsistent_workers)\n",
    "malicious_workers = normalized(malicious_workers)\n",
    "suspicious_workers = normalized(suspicious_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report top bad workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format_xyz_workers(xyz_workers, reverse=True):\n",
    "    return '\\n'.join('worker_id: %s; value: %.2f' % (k, v) for k, v in sorted(\n",
    "        xyz_workers.iteritems(), key=lambda p: p[1], reverse=reverse)[:10],)\n",
    "\n",
    "print 'Inconsistent:'\n",
    "print format_xyz_workers(inconsistent_workers)\n",
    "\n",
    "print '-' * 80\n",
    "print 'Malicious:'\n",
    "print format_xyz_workers(malicious_workers)\n",
    "\n",
    "print '-' * 80\n",
    "print 'Both:', set(inconsistent_workers.iterkeys()) & set(malicious_workers.iterkeys())\n",
    "\n",
    "print '-' * 80\n",
    "print 'Suspicious:'\n",
    "print format_xyz_workers(suspicious_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide who is spammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = collections.defaultdict(lambda: {})\n",
    "item_scores = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "potential_bad_abandonments = collections.defaultdict(lambda: {'num': 0, 'denom': 0})\n",
    "\n",
    "aggregated = {}\n",
    "num_skipped_labels = 0\n",
    "skipped_workers = set()\n",
    "worker_trust = {}\n",
    "for row in rows:\n",
    "    worker_id = row['_worker_id']\n",
    "    log_id = row['log_id']\n",
    "    if workers[worker_id] < MIN_JUDGEMENTS_PER_WORKER \\\n",
    "            or SKIP_INCONSISTENT_WORKERS and worker_id in inconsistent_workers \\\n",
    "            or malicious_workers[worker_id] > MALICIOUS_WORKER_THRESHOLD \\\n",
    "            or suspicious_workers[worker_id] > SUSPICIOUS_WORKER_THRESHOLD:\n",
    "        num_skipped_labels += 1\n",
    "        skipped_workers.add(worker_id)\n",
    "        continue\n",
    "    try:\n",
    "        relevance = int(row[rel_column[MODE]][1:])\n",
    "    except ValueError:\n",
    "        relevance = -4\n",
    "    if relevance == 0:\n",
    "        potential_bad_abandonments[row[orig_query[MODE]]]['denom'] += 1\n",
    "        if row.get('no_detailed') == 'D0.1' or 'bad_abandonment' in row['query'].split():\n",
    "            potential_bad_abandonments[row[orig_query[MODE]]]['num'] += 1\n",
    "    trust = float(row['_trust'])\n",
    "    worker_trust[worker_id] = trust\n",
    "    item_scores[log_id][relevance] += trust if USE_CF_TRUST else 1\n",
    "    labels[worker_id][log_id] = relevance\n",
    "\n",
    "    if DUMP_AGGREGATED_SCORES:\n",
    "        aggregated.setdefault(log_id,\n",
    "                {'query': row[orig_query[args.mode]], 'url': row['link'],\n",
    "                 'text': log_id_to_snippet_text[log_id],\n",
    "                 'r': {}, 'ambiguous': 0, 'bad_abandonment': 0})\n",
    "        aggregated[log_id]['r'].setdefault(relevance, 0)\n",
    "        aggregated[log_id]['r'][relevance] += 1\n",
    "        aggregated[log_id]['ambiguous'] += 'ambiguous' in row['query'].split()\n",
    "        aggregated[log_id]['bad_abandonment'] += 'bad_abandonment' in row['query'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d workers should be skipped' % len(skipped_workers)\n",
    "global_spammers_here = set(workers.iterkeys()) & spammers\n",
    "print '%d global spammers found' % len(global_spammers_here)\n",
    "print '%d of global spammers are not counted here' % len(global_spammers_here - skipped_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the [Crowdtruth](http://crowdtruth.org/)-style scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clarity_scores = {k: max(v.itervalues()) / sum(v.itervalues()) for k, v in item_scores.iteritems()}\n",
    "\n",
    "min_score = np.mean(clarity_scores.values()) - np.std(clarity_scores.values())\n",
    "unclear_items = set(\n",
    "        (log_id for log_id, score in clarity_scores.iteritems() if score < min_score))\n",
    "\n",
    "if not USE_CF_TRUST:\n",
    "    worker_item_scores = {}\n",
    "    worker_worker_scores = {}\n",
    "    for worker_id, w_labels in labels.iteritems():\n",
    "        worker_cosines = []\n",
    "        for log_id, r in w_labels.iteritems():\n",
    "            if SKIP_UNCLEAR_ITEMS and log_id in unclear_items:\n",
    "                continue\n",
    "            item_score = copy.copy(item_scores[log_id])\n",
    "            # exclude the current worker:\n",
    "            item_score[r] -= 1\n",
    "            # normalize\n",
    "            denom = sum(item_score.itervalues())\n",
    "            if denom > 0:\n",
    "                worker_cosines.append(item_score[r] / denom)\n",
    "        worker_item_scores[worker_id] = np.mean(worker_cosines)\n",
    "        # Now, to worker-worker scores.\n",
    "        worker_worker_cosines = []\n",
    "        for worker_id2, w_labels2 in labels.iteritems():\n",
    "            cosine_w_w2 = []\n",
    "            if worker_id2 == worker_id:\n",
    "                continue\n",
    "            for log_id, r in w_labels.iteritems():\n",
    "                if (SKIP_UNCLEAR_ITEMS and log_id in unclear_items) or log_id not in w_labels2:\n",
    "                    continue\n",
    "                cosine_w_w2.append(1.0 if w_labels2[log_id] == r else 0.0)\n",
    "            if len(cosine_w_w2) > 0:\n",
    "                worker_worker_cosines.append(np.mean(cosine_w_w2))\n",
    "        worker_worker_scores[worker_id] = np.mean(worker_worker_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Lowest worker-item scores:'\n",
    "print format_xyz_workers(worker_item_scores, reverse=False)\n",
    "\n",
    "print '-' * 80\n",
    "print 'Lowest worker-worker scores:'\n",
    "print format_xyz_workers(worker_worker_scores, reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the worker-worker and worker-item scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([worker_item_scores.get(w, 0) for w in workers])\n",
    "Y = np.array([worker_worker_scores.get(w, 0) for w in workers])\n",
    "colors = np.array([('red' if w in skipped_workers | global_spammers_here else 'green') for w in workers])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Agreement Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_W_I_SCORE = 0.0\n",
    "MIN_W_W_SCORE = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_values = []\n",
    "filtered_workers = set()\n",
    "w_filtered = len(skipped_workers)\n",
    "l_filtered = num_skipped_labels\n",
    "num_labels = num_skipped_labels\n",
    "for w in workers:\n",
    "    if w in skipped_workers:\n",
    "        continue\n",
    "    num_labels += len(labels[w])\n",
    "    if (w in global_spammers_here or \n",
    "            worker_item_scores[w] < MIN_W_I_SCORE or\n",
    "            worker_worker_scores[w] < MIN_W_W_SCORE):\n",
    "        l_filtered += len(labels[w])\n",
    "        w_filtered += 1\n",
    "        filtered_workers.add(w)\n",
    "    else:\n",
    "        l_values.append(labels[w])\n",
    "\n",
    "print '%d workers filtered out of %d (%.1f%%)' % (\n",
    "        w_filtered, len(workers), w_filtered / len(workers) * 100)\n",
    "print '%d labels filtered out of %d (%.1f%%)' % (\n",
    "        l_filtered, num_labels, l_filtered / num_labels * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Average Cohen\\'s kappa: %f' % cohen_kappa.cohen_kappa(l_values, missing_functor=lambda x: x < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Krippendorf\\'s alpha: %f' % krippendorff_alpha.krippendorff_alpha(l_values, missing_functor=lambda x: x < 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "################################################################################\n",
    "#\n",
    "# Notebook to do various data slicing after collecting ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from __future__ import division\n",
    "\n",
    "import bs4\n",
    "import collections\n",
    "import csv\n",
    "import itertools\n",
    "import jsonpickle\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "import sklearn.cross_validation\n",
    "\n",
    "from pyclick.click_models.PBM import PBM as pyclick_PBM\n",
    "from pyclick.search_session.SearchResult import SearchResult as pyclick_SearchResult\n",
    "from pyclick.search_session.SearchSession import SearchSession as pyclick_SearchSession\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('__file__'), os.path.pardir)))\n",
    "\n",
    "from logs_processing.click_model import *\n",
    "from logs_processing.create_tasks import Action, LogItem\n",
    "from logs_processing.fields import orig_query, rel_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF = '<DIRECTORY_WITH_EXPORTED_CROWD_FLOWER_RESULT_CSVs>'\n",
    "SPAMMER_FILENAMES = ['spammer1.txt', 'spammer2.txt']\n",
    "RESULTS_D = 'f789260_D.full.csv'\n",
    "RESULTS_AR = ['f842336_A+R.part1.csv', 'f845369_A+R.part2.csv', 'f845808_A+R.part3.csv', 'f846814_A+R.part4.csv']\n",
    "TASK_FILE = 'task.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CF_TRUST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the spammers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spammers = set()\n",
    "for s_file_name in SPAMMER_FILENAMES:\n",
    "    with open(os.path.join(CF, s_file_name)) as f:\n",
    "        for worker_id in f:\n",
    "            spammers.add(worker_id.rstrip())\n",
    "\n",
    "print '%d spammers' % len(spammers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_id_to_rel = collections.defaultdict(RelContainer)\n",
    "log_id_to_query = {}\n",
    "good_worker_ratings = 0\n",
    "total_ratings = 0\n",
    "d_workers = set()\n",
    "with open(os.path.join(CF, RESULTS_D)) as results_D:\n",
    "    for row in csv.DictReader(results_D):\n",
    "        worker_id = row['_worker_id']\n",
    "        d_workers.add(worker_id)\n",
    "        total_ratings += 1\n",
    "        if worker_id not in spammers:\n",
    "            good_worker_ratings += 1\n",
    "            trust = float(row['_trust']) if USE_CF_TRUST else 1\n",
    "            log_id = row['log_id']\n",
    "            RelContainer.add_rel(log_id_to_rel[log_id].Ds, row[rel_column['D']], trust)\n",
    "            log_id_to_query[log_id] = row[orig_query['D']]\n",
    "print '(D) %.1f%% ratings form spammers' % (100 - 100 * good_worker_ratings / total_ratings)\n",
    "print '(D) workers', len(d_workers)   \n",
    "print '(D) ratings', total_ratings\n",
    "    \n",
    "good_worker_ratings = 0\n",
    "total_ratings = 0\n",
    "r_workers = set()\n",
    "for result_AR in RESULTS_AR:\n",
    "    with open(os.path.join(CF, result_AR)) as results_AR:\n",
    "        for row in csv.DictReader(results_AR):\n",
    "            worker_id = row['_worker_id']\n",
    "            r_workers.add(worker_id)\n",
    "            total_ratings += 1\n",
    "            if worker_id not in spammers:\n",
    "                good_worker_ratings +=1\n",
    "                trust = float(row['_trust']) if USE_CF_TRUST else 1\n",
    "                log_id = row['log_id']\n",
    "                RelContainer.add_rel(log_id_to_rel[log_id].Rs, row[rel_column['R']], trust)\n",
    "                query = row[orig_query['R']]\n",
    "                old_query = log_id_to_query.setdefault(log_id, query)\n",
    "                if old_query != query:\n",
    "                    print >>sys.stderr, ('The same log_id '\n",
    "                            '(%s) maps to two different queries: [%s] and [%s]' % (\n",
    "                                    log_id, old_query, query))\n",
    "                    sys.exit(1)\n",
    "\n",
    "print '%d items with complete relevance' % sum(\n",
    "        1 for r in log_id_to_rel.itervalues() if r)\n",
    "\n",
    "print '%d queries with at least one completely judged document' % len(set(\n",
    "        log_id_to_query[k] for k, r in log_id_to_rel.iteritems() if r))\n",
    "\n",
    "print '(R) workers', len(r_workers)\n",
    "print '(R) ratings', total_ratings\n",
    "\n",
    "print '(R) %.1f%% ratings form spammers' % (100 - 100 * good_worker_ratings / total_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the SERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(os.path.join(CF, TASK_FILE)) as task_file:\n",
    "    sat_labels = []\n",
    "    num_skipped = 0\n",
    "    num_sat_true = 0\n",
    "    num_total = 0\n",
    "    reader = csv.DictReader(task_file)\n",
    "    for key, query_rows_iter in itertools.groupby(reader,\n",
    "                    key=lambda row: (row['log_id'].split('_')[:-1], # SERP id\n",
    "                                     row[orig_query['query']],\n",
    "                                     row['sat_feedback'])):\n",
    "        sat = key[2]\n",
    "        if sat == 'undefined':\n",
    "            print >>sys.stderr, 'Undefined sat label for query [%s]' % query\n",
    "        sat_labels.append(sat)\n",
    "        sat = parse_sat(sat)\n",
    "        if sat is None:\n",
    "            num_skipped += 1\n",
    "            continue\n",
    "        elif sat:\n",
    "            num_sat_true += 1\n",
    "        data_row = {'query': key[1], 'sat': sat, 'session': [], 'serp': []}\n",
    "        for row in query_rows_iter:\n",
    "            data_row['session'].append(jsonpickle.decode(row['actions']))\n",
    "            data_row['serp'].append(\n",
    "                    bs4.BeautifulSoup(row['snippet'], 'html.parser').li)\n",
    "        data.append(data_row)\n",
    "        num_total += 1\n",
    "    print collections.Counter(sat_labels)\n",
    "    print 'Skipped %d rows out of %d' % (num_skipped, num_total + num_skipped)\n",
    "    print '%.1f%% of SAT labels in the data' % (num_sat_true / num_total * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine CSS classes and geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classes = []\n",
    "min_geo_features = sys.maxint * np.ones(4)\n",
    "max_geo_features = -1* np.ones(4)\n",
    "offsets_left = []\n",
    "for i, d in enumerate(data):\n",
    "    for s in d['serp']:\n",
    "        all_classes.append(frozenset(s['class']))\n",
    "        geo_features = [int(f) for f in s['emup'].split(';')[1:]]\n",
    "        offsets_left.append(geo_features[0])\n",
    "        min_geo_features = np.minimum(min_geo_features, geo_features)\n",
    "        max_geo_features = np.maximum(max_geo_features, geo_features)\n",
    "print 'classes:', collections.Counter(all_classes)\n",
    "print 'min:', min_geo_features\n",
    "print 'max:', max_geo_features\n",
    "print 'offsets left:', collections.Counter(offsets_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classes = []\n",
    "one_classes = []\n",
    "for i, d in enumerate(data):\n",
    "    for s in d['session']:\n",
    "        print s.actions\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
